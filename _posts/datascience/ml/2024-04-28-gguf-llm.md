---
title: GGUF 파일로 로컬에서 LLM 실행하기
categories: ml
tags: llm
header:
  overlay_image: /assets/img/wallpaper.jpg
  overlay_filter: 0.2 # same as adding an opacity of 0.5 to a black background
---

최근에 Meta의 Llama3와 MS의 Phi-3 같은 고성능 경량화 언어 모델이 허깅페이스(HuggingFace)에서 오픈소스로 공개되었다. 이러한 모델은 ChatGPT나 Claude와 같은 대형 언어 모델(LLM)을 API로 사용하는 대안으로 고려될 수 있다. 오픈소스 모델을 로컬 환경에서 쉽고 빠르게 실행하려면, GGUF라는 파일 형식이 주로 사용된다는 사실을 알아두면 좋다. 그럼 GGUF에 대해 간략히 알아보고, 이를 사용해 Llama3 모델을 로컬 환경에서 실행하는 방법에 대해 살펴보자.

<br>

## GGUF(Georgi Gerganov Unified Format) 소개

GGUF는 GGML을 사용하여 대형 모델을 실행하는 프로그램과 모델을 저장하는 파일 형식이다. 참고로 [GGML](https://ggml.ai/)은 보통 컴퓨터에서도 큰 모델을 빠르게 돌릴 수 있는 ML용 라이브러리이다. GGUF는 모델을 빠르고 쉽게 불러오고 저장할 수 있게 해주는 바이너리 형식으로 설계되었다. 바이너리 형식이란 정보를 0과 1의 조합으로 표현하는 컴퓨터만 읽을 수 있는 형식을 말한다. 개발자들은 보통 PyTorch 같은 프로그래밍 도구를 사용해 모델을 만든 후, GGML에서 쓸 수 있도록 GGUF 형식으로 저장한다. GGUF는 이전에 사용되던 GGML, GGMF, GGJT와 같은 형식을 개선하여 모든 필요한 정보를 담고 있으며, 새로운 정보를 추가해도 기존 모델과 잘 맞도록 확장성을 가지고 설계되었다고 한다.

<br>

## GGUF 파일 구조

![png](/assets/img/post_img/2024-04-28-gguf-llm/0.png)

[@mishig25](https://github.com/mishig25)가 작성한 GGUFv3 다이어그램(위 그림)을 참고하면, GGUF 파일 구조를 대략적으로 파악할 수 있다. 모델 관련 메타 데이터(이름, 구조, 차원, 타입, 오프셋 등)는 Key Value 페어로, 텐서 정보(구조, 이름, 컨텍스트 길이, 파일타입, 토큰정보 등)는 값으로 작성된다는 것을 알 수 있다.

<br>

## 허깅페이스에서 Llama3 GGUF 파일 다운로드

얼마전, Meta([@meta-llama](https://huggingface.co/meta-llama))에서 Meta-Llama-3-8B를 공개했는데, 이준범([@beomi](https://huggingface.co/beomi))님이 이를 기반으로 사전학습한 모델인 Llama-3-Open-Ko-8B를 공개했다. 해당 모델은 허깅페이스에 safetensors 파일 형식으로 업로드되어 있는데, 이를 곧바로 GGUF 파일 형식으로 변환한 모델을 이경록([@teddylee777](https://huggingface.co/teddylee777))님이 공개했다. 너무나 감사하게도 두 분께서 활약해주신 덕분에 한국어를 지원하는 Llama3 모델을 이제 누구나 손 쉽게 비교적 저사양 컴퓨터에서도 빠르게 실행해 볼 수 있다.

다음의 과정을 거치면 Llama-3-Open-Ko-8B-gguf 모델을 로컬에 다운로드할 수 있다.

- 허깅페이스 저장소 접속

https://huggingface.co/teddylee777/Llama-3-Open-Ko-8B-gguf

![png](/assets/img/post_img/2024-04-28-gguf-llm/1.png)

- Files 탭 - Q8_0.gguf 파일 다운로드 아이콘 클릭

![png](/assets/img/post_img/2024-04-28-gguf-llm/2.png)

<br>

## Ollama 설치

Ollama는 LLM을 쉽게 실행할 수 있도록 하는 오픈소스이다. Ollama에서 공식적으로 지원하는 일부 모델(llama3, phi3 등)을 간단한 명령으로 설치/실행할 수 있다. 물론 위에서 다운로드한 GGUF 파일을 직접 설치/실행할 수 있다.

- 다음 링크 접속 후 Ollama를 다운로드 후 설치

https://ollama.com/

![png](/assets/img/post_img/2024-04-28-gguf-llm/3.png)

<br>

## Modelfile 작성

Ollama에서 지원하는 공식 모델이 아니라 허깅페이스 등에서 다운받은 커스텀 LLM 모델의 경우 아래와 같이 Modelfile 파일을 작성하여 수동으로 모델을 추가해야 한다.

- GGUF 파일 동일 경로에 Modelfile 파일 작성

```yaml
{% raw %}
FROM Llama-3-Open-Ko-8B-Q8_0.gguf

TEMPLATE """{{- if .System }}
<s>{{ .System }}</s>
{{- end }}
<s>Human:
{{ .Prompt }}</s>
<s>Assistant:
"""

SYSTEM """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."""

PARAMETER temperature 0
PARAMETER num_predict 3000
PARAMETER num_ctx 4096
PARAMETER stop <s>
PARAMETER stop </s>
{% endraw %}
```

<br>

## Llama3 실행

GGUF와 Modelfile 두 개의 파일이 준비되었다면, 이제 CMD에서 모델을 생성하고 실행할 수 있다.

- 모델 추가 명령

```bash
ollama create llama3-ko -f Modelfile
```

- 모델 실행 명령

```bash
ollama run llama3-ko
```